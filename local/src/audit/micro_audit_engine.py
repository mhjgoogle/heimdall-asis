#!/usr/bin/env python3
"""
Micro Audit Engine - ç‰ˆæœ¬åŒ–çš„ Vic è¶‹åŠ¿çº¿å’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¼•æ“

æ”¯æŒ Experimental å’Œ Production ä¸¤ç§æ¨¡å¼ï¼Œå…è®¸ç‰ˆæœ¬å¯¹æ¯”å’Œé€»è¾‘è¿­ä»£ã€‚
"""

import argparse
import json
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Generator
import pandas as pd
import numpy as np
from abc import ABC, abstractmethod


# ==================== æ ¸å¿ƒè·¯å¾„é…ç½® ====================
PROJECT_ROOT = Path(__file__).parent.parent.parent
DB_PATH = PROJECT_ROOT / "data" / "heimdall.db"
CACHE_DIR = PROJECT_ROOT / "data" / "cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)


# ==================== ç­–ç•¥ç‰ˆæœ¬æ§åˆ¶ ====================
class StrategyVersion:
    """ç­–ç•¥ç‰ˆæœ¬ç®¡ç†"""

    # ç”Ÿäº§ç‰ˆæœ¬ï¼ˆç¡®å®šçš„ TradeVic36 é€»è¾‘ï¼‰
    PRODUCTION = "v1_tradervic36"

    # å®éªŒç‰ˆæœ¬ï¼ˆå¾…éªŒè¯çš„æ–°é€»è¾‘ï¼‰
    EXPERIMENTAL = "v2_beta"

    @staticmethod
    def get_all_versions() -> List[str]:
        return [StrategyVersion.PRODUCTION, StrategyVersion.EXPERIMENTAL]


# ==================== ç­–ç•¥æ³¨å†Œè¡¨ ====================
class StrategyRegistry:
    """ç­–ç•¥æ’ä»¶æ³¨å†Œè¡¨ - æ”¯æŒåŠ¨æ€æ·»åŠ æ–°çš„è®¡ç®—é€»è¾‘"""

    _strategies = {}

    @classmethod
    def register(cls, name: str, strategy_class: type):
        """æ³¨å†Œæ–°ç­–ç•¥"""
        cls._strategies[name] = strategy_class

    @classmethod
    def get_strategy(cls, name: str, version: str = StrategyVersion.PRODUCTION):
        """è·å–ç­–ç•¥å®ä¾‹"""
        if name not in cls._strategies:
            raise ValueError(f"Strategy '{name}' not registered")
        return cls._strategies[name](version=version)

    @classmethod
    def list_strategies(cls) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æ³¨å†Œçš„ç­–ç•¥"""
        return list(cls._strategies.keys())


# ==================== æŠ€æœ¯æŒ‡æ ‡ç­–ç•¥åŸºç±» ====================
class IndicatorStrategy(ABC):
    """æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ç­–ç•¥åŸºç±»"""
    
    def __init__(self, version: str = StrategyVersion.PRODUCTION):
        self.version = version
        self.config = self._load_config()
    
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®å‚æ•° - æ”¯æŒç‰ˆæœ¬ç‰¹å®šçš„é…ç½®"""
        base_config = {
            'atr_period': 14,
            'atr_multiplier': 1.5,
            'sma_periods': [20, 60, 200],
            'bias_period': 200,
            'consolidation_window': 20,
            'consolidation_threshold': 0.02,
            'window_2month': 60,
            'window_1year': 250,
            'window_3year': 750,
            'min_span_short': 3,
            'min_span_mid': 15,
            'min_span_long': 30,
            'atr_multiplier_touch': 0.5,
            'fallback_tolerance_pct': 0.005,
            'recent_days_threshold': 125,
            'group_threshold_short': 10,
            'group_threshold_long': 60,
            'edge_window': 30,
        }

        # å®éªŒç‰ˆæœ¬é…ç½®è°ƒæ•´
        if self.version == StrategyVersion.EXPERIMENTAL:
            # v2å®éªŒç‰ˆæœ¬ï¼šæ›´ä¸¥æ ¼çš„è§¦ç‚¹éªŒè¯ï¼Œå‡å°‘å‡ä¿¡å·
            base_config.update({
                'atr_multiplier_touch': 0.3,  # é™ä½å®¹å·®ï¼Œæ›´ä¸¥æ ¼
                'min_span_short': 5,  # è¦æ±‚æ›´é•¿çš„æœ€å°è·¨åº¦
                'consolidation_threshold': 0.015,  # æ›´ä½çš„ç›˜æ•´é˜ˆå€¼
            })

        return base_config
    
    @abstractmethod
    def calculate(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ‰§è¡Œè®¡ç®—ï¼Œè¿”å›ç»“æœå­—å…¸"""
        pass


# ==================== Vic è¶‹åŠ¿çº¿ç­–ç•¥ ====================
class VicTrendStrategy(IndicatorStrategy):
    """Vic è¶‹åŠ¿çº¿è¯†åˆ«ç­–ç•¥"""
    
    def calculate(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®— Vic è¶‹åŠ¿çº¿ - åŒ¹é…TradeVic36"""
        df = df.copy()

        # å‡†å¤‡æ•°æ® - åŸºäºCandle Body (Open/Close)
        df['BodyHigh'] = df[['Open', 'Close']].max(axis=1)
        df['BodyLow'] = df[['Open', 'Close']].min(axis=1)
        df['ATR'] = self._calculate_atr(df)

        # è¯†åˆ«é”šç‚¹
        anchors = self._identify_anchors(df)

        # ç”Ÿæˆè¶‹åŠ¿çº¿
        trendlines = list(self._generate_lines(df, anchors))

        return {
            'version': self.version,
            'timestamp': datetime.now().isoformat(),
            'anchors': [{'date': str(a['date']), 'type': a['type'], 'period': a['period']} for a in anchors],
            'trendlines': trendlines,
            'consolidation': self._check_consolidation(df),
            'metadata': {
                'data_length': len(df),
                'date_range': f"{df.index[0].date()} ~ {df.index[-1].date()}",
                'atr_mean': df['ATR'].mean(),
            }
        }
    
    def _calculate_atr(self, df: pd.DataFrame) -> pd.Series:
        """è®¡ç®— ATRï¼ˆAverage True Rangeï¼‰- åŒ¹é…TradeVic36"""
        period = self.config['atr_period']
        high = df['High']
        low = df['Low']
        close = df['Close']
        prev_close = close.shift(1)

        tr1 = high - low
        tr2 = (high - prev_close).abs()
        tr3 = (low - prev_close).abs()

        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(window=period).mean()

        # å¡«å……NaN
        atr = atr.bfill()

        mask = atr.isna()
        if mask.any():
            atr[mask] = close[mask] * 0.01

        return atr
    
    def _check_consolidation(self, df: pd.DataFrame) -> Dict:
        """è¯†åˆ«æ¨ªç›˜æ•´ç†"""
        window = self.config['consolidation_window']
        threshold = self.config['consolidation_threshold']
        
        if len(df) < window:
            return {'is_active': False, 'reason': 'insufficient_data'}
        
        recent = df.tail(window)
        high = recent['BodyHigh'].max()
        low = recent['BodyLow'].min()
        volatility = (high - low) / recent['Close'].mean() if recent['Close'].mean() != 0 else 0
        
        return {
            'is_active': volatility < threshold,
            'volatility': volatility,
            'threshold': threshold,
        }
    
    def _identify_anchors(self, df: pd.DataFrame) -> List[Dict]:
        """è¯†åˆ«é”šç‚¹ - åŒ¹é…TradeVic36é€»è¾‘"""
        anchors = []

        # 1. å…¨å±€æå€¼
        global_max_idx = df['BodyHigh'].idxmax()
        global_min_idx = df['BodyLow'].idxmin()

        if pd.notna(global_max_idx):
            anchors.append({'date': global_max_idx, 'type': 'down', 'period': 'Global'})
        if pd.notna(global_min_idx):
            anchors.append({'date': global_min_idx, 'type': 'up', 'period': 'Global'})

        # 2. æ»‘åŠ¨çª—å£
        windows = [50, 250, 750]
        for w in windows:
            if w >= 700:
                p_name = '3Year'
            elif w >= 200:
                p_name = '1Year'
            else:
                p_name = '2Month'

            rolling_max = df['BodyHigh'].rolling(window=w, center=True).max()
            high_points = df[df['BodyHigh'] == rolling_max]
            for date in high_points.index:
                anchors.append({'date': date, 'type': 'down', 'period': p_name})

            rolling_min = df['BodyLow'].rolling(window=w, center=True).min()
            low_points = df[df['BodyLow'] == rolling_min]
            for date in low_points.index:
                anchors.append({'date': date, 'type': 'up', 'period': p_name})

        # 3. è¾¹ç¼˜è¡¥å…¨
        last_window = self.config['edge_window']
        if len(df) > last_window:
            recent_df = df.iloc[-last_window:]

            recent_high_date = recent_df['BodyHigh'].idxmax()
            is_new = True
            for a in anchors:
                if a['date'] == recent_high_date and a['type'] == 'down':
                    is_new = False; break
            if is_new and pd.notna(recent_high_date):
                anchors.append({'date': recent_high_date, 'type': 'down', 'period': '2Month'})

            recent_low_date = recent_df['BodyLow'].idxmin()
            is_new = True
            for a in anchors:
                if a['date'] == recent_low_date and a['type'] == 'up':
                    is_new = False; break
            if is_new and pd.notna(recent_low_date):
                anchors.append({'date': recent_low_date, 'type': 'up', 'period': '2Month'})

        # 4. 10å¹´æ—¶é—´è¿‡æ»¤
        max_years = 10  # self.config.get('max_anchor_years', 10)
        cutoff_date = df.index[-1] - pd.Timedelta(days=max_years * 365)

        filtered_anchors = [a for a in anchors if a['date'] >= cutoff_date]

        # 5. å»é‡
        unique_anchors = {}
        priority = {'Global': 4, '3Year': 3, '1Year': 2, '2Month': 1}
        filtered_anchors.sort(key=lambda x: priority.get(x['period'], 0))

        for a in filtered_anchors:
            key = (a['date'], a['type'])
            unique_anchors[key] = a

        return sorted(list(unique_anchors.values()), key=lambda x: x['date'])
    
    def _generate_lines(self, df: pd.DataFrame, anchors: List[Dict]) -> Generator[Dict, None, None]:
        """ç”Ÿæˆè¶‹åŠ¿çº¿ - åŒ¹é…TradeVic36çš„å®Œæ•´é€»è¾‘"""
        raw_lines = []

        rec_thresh_days = self.config.get('recent_days_threshold', 125)
        if len(df) > rec_thresh_days:
            recent_threshold = df.index[-rec_thresh_days]
        else:
            recent_threshold = df.index[0]

        priority_map = {'Global': 4, '3Year': 3, '1Year': 2, '2Month': 1}

        for i, anchor in enumerate(anchors):
            if anchor['period'] == '2Month' and anchor['date'] < recent_threshold:
                continue

            target_date = None
            current_priority = priority_map.get(anchor['period'], 0)

            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªç›¸åç±»å‹çš„é”šç‚¹
            for j in range(i + 1, len(anchors)):
                next_anchor = anchors[j]
                if next_anchor['type'] != anchor['type']:
                    if anchor['period'] == '2Month' or \
                       priority_map.get(next_anchor['period'], 0) >= current_priority:
                        target_date = next_anchor['date']
                        break

            line = self._generate_segment(df, anchor, target_date)
            if line:
                raw_lines.append(line)

        if not raw_lines:
            return []

        raw_lines.sort(key=lambda x: x['p1'][0])
        curr_group = [raw_lines[0]]

        for l in raw_lines[1:]:
            prev = curr_group[-1]
            time_diff = (l['p1'][0] - prev['p1'][0]).days

            thresh = self.config['group_threshold_short'] if l['period'] == '2Month' else self.config['group_threshold_long']

            if time_diff < thresh and l['type'] == prev['type']:
                curr_group.append(l)
            else:
                best_line = self._select_best_line(curr_group)
                if best_line:
                    # è½¬æ¢ä¸ºè¾“å‡ºæ ¼å¼
                    yield {
                        'start_date': str(best_line['p1'][0].date()),
                        'start_price': best_line['p1'][1],
                        'break_date': str(best_line['break_date'].date()),
                        'break_price': best_line['p2'][1],
                        'type': best_line['type'],
                        'period': best_line['period'],
                        'slope': best_line['slope'],
                        'touches': best_line['touch_count'],
                        'strength': 'strong' if best_line['touch_count'] >= 3 else 'weak',
                    }
                curr_group = [l]

        if curr_group:
            best_line = self._select_best_line(curr_group)
            if best_line:
                yield {
                    'start_date': str(best_line['p1'][0].date()),
                    'start_price': best_line['p1'][1],
                    'break_date': str(best_line['break_date'].date()),
                    'break_price': best_line['p2'][1],
                    'type': best_line['type'],
                    'period': best_line['period'],
                    'slope': best_line['slope'],
                    'touches': best_line['touch_count'],
                    'strength': 'strong' if best_line['touch_count'] >= 3 else 'weak',
                }

    def _generate_segment(self, df: pd.DataFrame, anchor: Dict, next_anchor_date=None) -> Optional[Dict]:
        """ç”Ÿæˆå•æ¡è¶‹åŠ¿çº¿ç‰‡æ®µ"""
        start_date = anchor['date']
        trend_type = anchor['type']
        anchor_period = anchor['period']

        if start_date not in df.index:
            return None

        sub_df = df.loc[start_date:].copy()
        if len(sub_df) < 5:
            return None

        dates = sub_df.index
        lows = sub_df['BodyLow'].values
        highs = sub_df['BodyHigh'].values
        closes = sub_df['Close'].values
        atrs = sub_df['ATR'].values

        if next_anchor_date and next_anchor_date in sub_df.index:
            target_idx = sub_df.index.get_loc(next_anchor_date)
        else:
            target_idx = np.argmax(highs) if trend_type == 'up' else np.argmin(lows)

        if target_idx < 2:
            target_idx = len(sub_df) - 1

        # å‡¸åŒ…æ‰«æ
        best_idx = -1
        scan_end = min(target_idx + 1, len(sub_df))
        scan_start = 1

        if trend_type == 'up':
            min_slope = np.inf
            for i in range(scan_start, scan_end):
                if i >= len(lows): break
                slope = (lows[i] - lows[0]) / i
                if slope < min_slope:
                    min_slope = slope
                    best_idx = i
            current_slope = min_slope
        else:
            max_slope = -np.inf
            for i in range(scan_start, scan_end):
                if i >= len(highs): break
                slope = (highs[i] - highs[0]) / i
                if slope > max_slope:
                    max_slope = slope
                    best_idx = i
            current_slope = max_slope

        if best_idx == -1 or np.isinf(current_slope):
            return None

        # Vic å‡†åˆ™éªŒè¯
        if trend_type == 'up' and current_slope <= 0:
            return None
        if trend_type == 'down' and current_slope >= 0:
            return None

        # æœ€å°è·¨åº¦è¿‡æ»¤
        min_span_req = self.config['min_span_short'] if anchor_period == '2Month' \
                      else (self.config['min_span_mid'] if anchor_period == '1Year' \
                           else self.config['min_span_long'])
        if best_idx < min_span_req:
            return None

        # å»¶é•¿ä¸çªç ´
        final_slope = current_slope
        y_start = lows[0] if trend_type == 'up' else highs[0]
        break_idx = len(sub_df) - 1

        for i in range(best_idx + 1, len(sub_df)):
            line_price = y_start + final_slope * i
            price_check = closes[i]
            if (trend_type == 'up' and price_check < line_price) or \
               (trend_type == 'down' and price_check > line_price):
                break_idx = i
                break

        # è§¦ç‚¹éªŒè¯
        touch_count = 0
        atr_multiplier = self.config['atr_multiplier_touch']
        fallback_tol = self.config.get('fallback_tolerance_pct', 0.005)

        for i in range(break_idx + 1):
            line_p = y_start + final_slope * i
            bar_p = lows[i] if trend_type == 'up' else highs[i]

            current_atr = atrs[i]
            if pd.isna(current_atr) or current_atr == 0:
                tolerance_val = line_p * fallback_tol
            else:
                tolerance_val = current_atr * atr_multiplier

            dist = abs(bar_p - line_p)
            if dist <= tolerance_val:
                touch_count += 1

        return {
            'p1': (dates[0], y_start),
            'p2': (dates[break_idx], closes[break_idx]),  # ä½¿ç”¨å®é™…æ”¶ç›˜ä»·è€Œä¸æ˜¯å¤–æ¨ä»·æ ¼
            'type': trend_type,
            'period': anchor_period,
            'slope': final_slope,
            'break_date': dates[break_idx],
            'touch_count': touch_count
        }

    def _select_best_line(self, group):
        """ä»ç»„ä¸­é€‰æ‹©æœ€ä½³è¶‹åŠ¿çº¿"""
        if not group:
            return None
        if group[0]['type'] == 'up':
            return min(group, key=lambda x: x['slope'])
        else:
            return max(group, key=lambda x: x['slope'])
    
    def _count_touches(self, df: pd.DataFrame, start_price: float, slope: float, atr: float) -> int:
        """è®¡ç®—è¶‹åŠ¿çº¿è§¦ç‚¹æ•°"""
        touches = 0
        tolerance = atr * self.config['atr_multiplier']
        
        for i, row in df.iterrows():
            line_price = start_price + slope * (i - df.index[0]).days
            bar_price = row['BodyLow'] if slope > 0 else row['BodyHigh']
            
            if abs(bar_price - line_price) <= tolerance:
                touches += 1
        
        return touches


# ==================== æŠ€æœ¯æŒ‡æ ‡ç­–ç•¥ ====================
class TechnicalIndicatorStrategy(IndicatorStrategy):
    """SMAã€Biasã€åŠ¨é‡ç­‰æŒ‡æ ‡è®¡ç®—"""
    
    def calculate(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡"""
        df = df.copy()
        
        # è®¡ç®— SMA
        sma_results = {}
        for period in self.config['sma_periods']:
            sma_results[f'sma_{period}'] = df['Close'].rolling(period).mean().tolist()
        
        # è®¡ç®— Biasï¼ˆä¸200æ—¥å‡çº¿çš„ä¹–ç¦»ç‡ï¼‰
        sma_200 = df['Close'].rolling(self.config['bias_period']).mean()
        bias = ((df['Close'] - sma_200) / sma_200 * 100).tolist()
        
        # è®¡ç®—å¹´åŒ–æ³¢åŠ¨ç‡
        daily_returns = df['Close'].pct_change()
        volatility = daily_returns.std() * np.sqrt(252)
        
        return {
            'version': self.version,
            'timestamp': datetime.now().isoformat(),
            'sma': sma_results,
            'bias': bias,
            'volatility': volatility,
            'bias_threshold_high': 20.0,
            'bias_threshold_low': -20.0,
            'metadata': {
                'data_length': len(df),
                'last_close': df['Close'].iloc[-1],
                'last_bias': bias[-1],
            }
        }


# ==================== ç­–ç•¥æ³¨å†Œ ====================
StrategyRegistry.register('vic_trends', VicTrendStrategy)
StrategyRegistry.register('technical_indicators', TechnicalIndicatorStrategy)


# ==================== å¾®è§‚å®¡è®¡å¼•æ“ä¸»ç±» ====================
class MicroAuditEngine:
    """å¾®è§‚èµ„äº§å®¡è®¡å’Œè®¡ç®—å¼•æ“"""
    
    def __init__(self):
        self.db_path = DB_PATH
        self.cache_dir = CACHE_DIR
    
    def load_asset_data(self, catalog_key: str) -> Optional[pd.DataFrame]:
        """ä»æ•°æ®åº“åŠ è½½èµ„äº§æ•°æ®"""
        try:
            conn = sqlite3.connect(self.db_path)
            query = f"""
            SELECT date, val_open, val_high, val_low, val_close, val_volume
            FROM timeseries_micro
            WHERE catalog_key = ?
            ORDER BY date
            """
            df = pd.read_sql_query(query, conn, params=(catalog_key,))
            conn.close()
            
            if df.empty:
                print(f"âŒ æœªæ‰¾åˆ°èµ„äº§æ•°æ®: {catalog_key}")
                return None
            
            # é‡å‘½ååˆ—ä»¥ä¾¿åç»­ä½¿ç”¨
            df.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']
            df['Date'] = pd.to_datetime(df['Date'])
            df.set_index('Date', inplace=True)
            
            # æ•°æ®æ¸…æ´—
            df = df.bfill().ffill()
            
            return df
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def calculate_all_indicators(self, df: pd.DataFrame, version: str) -> Dict:
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆVic è¶‹åŠ¿çº¿ + æŠ€æœ¯æŒ‡æ ‡ï¼‰"""

        results = {
            'catalog_key': None,
            'version': version,
            'timestamp': datetime.now().isoformat(),
            'logic_version': version,  # è®°å½•è®¡ç®—é€»è¾‘ç‰ˆæœ¬
            'vic_trends': {},
            'technical_indicators': {},
        }

        # ä½¿ç”¨ç­–ç•¥æ³¨å†Œè¡¨æ‰§è¡Œè®¡ç®—
        vic_strategy = StrategyRegistry.get_strategy('vic_trends', version=version)
        results['vic_trends'] = vic_strategy.calculate(df)

        tech_strategy = StrategyRegistry.get_strategy('technical_indicators', version=version)
        results['technical_indicators'] = tech_strategy.calculate(df)

        return results
    
    def save_results(self, catalog_key: str, results: Dict, mode: str = 'production'):
        """ä¿å­˜è®¡ç®—ç»“æœåˆ°ç¼“å­˜"""
        cache_file = self.cache_dir / f"{catalog_key}_{mode}.json"
        
        # åºåˆ—åŒ–ç»“æœï¼ˆå¤„ç† numpy/datetime å¯¹è±¡ï¼‰
        serializable = self._make_serializable(results)
        
        with open(cache_file, 'w') as f:
            json.dump(serializable, f, indent=2, default=str)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜: {cache_file}")
        return cache_file
    
    def _make_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸º JSON åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(v) for v in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, (datetime, pd.Timestamp)):
            return str(obj)
        elif pd.isna(obj):
            return None
        else:
            return obj
    
    def process_asset(self, catalog_key: str, mode: str = 'production'):
        """å¤„ç†å•ä¸ªèµ„äº§"""
        print(f"\nğŸ“Š å¤„ç†èµ„äº§: {catalog_key} (æ¨¡å¼: {mode})")
        
        # åŠ è½½æ•°æ®
        df = self.load_asset_data(catalog_key)
        if df is None:
            return False
        
        print(f"  âœ“ æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡è®°å½•")
        
        # ç¡®å®šç‰ˆæœ¬
        version = StrategyVersion.EXPERIMENTAL if mode == 'experimental' else StrategyVersion.PRODUCTION
        
        # è®¡ç®—æŒ‡æ ‡
        results = self.calculate_all_indicators(df, version)
        results['catalog_key'] = catalog_key
        
        # ä¿å­˜ç»“æœ
        self.save_results(catalog_key, results, mode=mode)
        
        # æ‰“å°æ‘˜è¦
        print(f"  âœ“ Vic è¶‹åŠ¿çº¿: {len(results['vic_trends'].get('trendlines', []))} æ¡")
        print(f"  âœ“ é”šç‚¹æ•°: {len(results['vic_trends'].get('anchors', []))}")
        print(f"  âœ“ ç‰ˆæœ¬: {version}")
        
        return True
    
    def batch_process(self, mode: str = 'production', ticker_filter: Optional[str] = None):
        """æ‰¹é‡å¤„ç†èµ„äº§"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # è·å–æ‰€æœ‰ Daily é¢‘ç‡çš„èµ„äº§
            query = "SELECT DISTINCT catalog_key FROM data_catalog WHERE update_frequency = 'Daily'"
            if ticker_filter and ticker_filter != 'ALL':
                query += f" AND catalog_key = '{ticker_filter}'"

            cursor.execute(query)
            assets = [row[0] for row in cursor.fetchall()]
            conn.close()

            if not assets:
                print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„èµ„äº§")
                return

            print(f"\nğŸ”„ æ‰¹é‡å¤„ç† {len(assets)} ä¸ªèµ„äº§ (æ¨¡å¼: {mode})")

            success, failed = 0, 0
            for asset in assets:
                try:
                    if self.process_asset(asset, mode=mode):
                        success += 1
                    else:
                        failed += 1
                except Exception as e:
                    print(f"âŒ {asset}: {str(e)[:80]}")
                    failed += 1

            print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ: {success} æˆåŠŸ, {failed} å¤±è´¥")

        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")

    def batch_audit_differences(self, limit: Optional[int] = None) -> Dict:
        """æ‰¹é‡å›æµ‹å·®å¼‚æŠ¥å‘Š - æ¯”è¾ƒç”Ÿäº§ç‰ˆå’Œå®éªŒç‰ˆç»“æœ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # è·å–æ‰€æœ‰ Daily é¢‘ç‡çš„èµ„äº§
            query = "SELECT DISTINCT catalog_key FROM data_catalog WHERE update_frequency = 'Daily'"
            cursor.execute(query)
            assets = [row[0] for row in cursor.fetchall()]
            conn.close()

            if not assets:
                print(f"âŒ æœªæ‰¾åˆ°èµ„äº§")
                return {}

            if limit:
                assets = assets[:limit]

            print(f"\nğŸ”„ ç”Ÿæˆå·®å¼‚æŠ¥å‘Š ({len(assets)} ä¸ªèµ„äº§)")

            differences = []
            processed = 0

            for asset in assets:
                try:
                    prod_file = self.cache_dir / f"{asset}_production.json"
                    exp_file = self.cache_dir / f"{asset}_experimental.json"

                    if not prod_file.exists() or not exp_file.exists():
                        continue

                    with open(prod_file) as f:
                        prod_data = json.load(f)
                    with open(exp_file) as f:
                        exp_data = json.load(f)

                    # æ¯”è¾ƒè¶‹åŠ¿çº¿æ•°é‡
                    prod_lines = len(prod_data.get('vic_trends', {}).get('trendlines', []))
                    exp_lines = len(exp_data.get('vic_trends', {}).get('trendlines', []))

                    # æ¯”è¾ƒå¼ºè¶‹åŠ¿çº¿æ•°é‡
                    prod_strong = sum(1 for t in prod_data.get('vic_trends', {}).get('trendlines', [])
                                     if t.get('strength') == 'strong')
                    exp_strong = sum(1 for t in exp_data.get('vic_trends', {}).get('trendlines', [])
                                    if t.get('strength') == 'strong')

                    # æ¯”è¾ƒé”šç‚¹æ•°é‡
                    prod_anchors = len(prod_data.get('vic_trends', {}).get('anchors', []))
                    exp_anchors = len(exp_data.get('vic_trends', {}).get('anchors', []))

                    differences.append({
                        'asset': asset,
                        'prod_trendlines': prod_lines,
                        'exp_trendlines': exp_lines,
                        'trendlines_diff': exp_lines - prod_lines,
                        'prod_strong_lines': prod_strong,
                        'exp_strong_lines': exp_strong,
                        'strong_lines_diff': exp_strong - prod_strong,
                        'prod_anchors': prod_anchors,
                        'exp_anchors': exp_anchors,
                        'anchors_diff': exp_anchors - prod_anchors,
                    })

                    processed += 1
                    if processed % 10 == 0:
                        print(f"  âœ“ å·²å¤„ç† {processed}/{len(assets)} ä¸ªèµ„äº§")

                except Exception as e:
                    print(f"âš ï¸ {asset} å¤„ç†å¤±è´¥: {str(e)[:50]}")
                    continue

            # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
            summary = {
                'total_assets': len(differences),
                'avg_trendlines_diff': np.mean([d['trendlines_diff'] for d in differences]),
                'avg_strong_lines_diff': np.mean([d['strong_lines_diff'] for d in differences]),
                'avg_anchors_diff': np.mean([d['anchors_diff'] for d in differences]),
                'details': differences,
            }

            print("\nâœ… å·®å¼‚æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            print(f"   èµ„äº§æ•°é‡: {summary['total_assets']}")
            print(f"   å¹³å‡è¶‹åŠ¿çº¿å·®å¼‚: {summary['avg_trendlines_diff']:.2f}")
            print(f"   å¹³å‡å¼ºè¶‹åŠ¿çº¿å·®å¼‚: {summary['avg_strong_lines_diff']:.2f}")
            print(f"   å¹³å‡é”šç‚¹å·®å¼‚: {summary['avg_anchors_diff']:.2f}")

            return summary

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå·®å¼‚æŠ¥å‘Šå¤±è´¥: {e}")
            return {}


# ==================== å‘½ä»¤è¡Œå…¥å£ ====================
def main():
    parser = argparse.ArgumentParser(description='å¾®è§‚å®¡è®¡å¼•æ“ - Vic è¶‹åŠ¿çº¿å’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—')
    parser.add_argument('--ticker', type=str, default='STOCK_PRICE_NVDA', 
                        help='èµ„äº§ä»£ç  (æ”¯æŒ ALL è¡¨ç¤ºå…¨éƒ¨)')
    parser.add_argument('--mode', type=str, choices=['production', 'experimental'], 
                        default='production', help='è¿è¡Œæ¨¡å¼')
    
    args = parser.parse_args()
    
    engine = MicroAuditEngine()
    
    if args.ticker == 'ALL':
        engine.batch_process(mode=args.mode)
    else:
        engine.process_asset(args.ticker, mode=args.mode)


if __name__ == '__main__':
    main()
